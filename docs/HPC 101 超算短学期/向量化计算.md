# 向量化计算

##  Part 01 NumPy

### 开始之前

**配环境：**

- 方法一：直接安装Python+pip安装NumPy
- 方法二：安装 Anaconda/Micromamba（更轻量）/uv（更快，文件夹级别的环境管理），使用对应的 install 进行安装（推荐）

### 准备好之后

在你习惯的环境中输入 import numpy as np

- 含义是，导入 numpy 这个包，并且给它起个名叫 np
- 以后需要这个包的东西，只需要 np.xxxx 即可调用
- 通常约定俗成就叫它 np
- 没有提示就是运行正常（特指 python shell）

### NumPy基础

**创建一个矩阵**

**Note:在NumPy中，默认的数据类型是float64**

- 将原有列表转为NumPy矩阵

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20250630141908618.png" alt="image-20250630141908618" style="zoom:50%;" />

- 创建一个全是0的矩阵，创建一个都是1的矩阵，参数为矩阵的形状（也可以更高维）

  <img src="C:\Users\Lenovo\Pictures\Screenshots\屏幕截图 2025-06-30 142054.png" alt="屏幕截图 2025-06-30 142054" style="zoom:50%;" /><img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20250630142103584.png" alt="image-20250630142103584" style="zoom:50%;" />

- 创建都是自定义值的矩阵，创建单位矩阵，创建空矩阵

  <img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20250630142259770.png" alt="image-20250630142259770" style="zoom:50%;" />

- 创建等步长矩阵

  <img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20250630142409121.png" alt="image-20250630142409121" style="zoom:50%;" />

- 创建随机数矩阵

  ![image-20250630142546479](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20250630142546479.png)

- 使用reshape()可以更改矩阵的形状，也可以用shape查看形状，用ndim查看维数

- 数据类型：当创建一个 NumPy 的 array 的时候，实际上创建了一个 ndarray 类型的对象，含义是 n-dimension array （n维数组）

- python list的实现：list里面存的是指针，指向真实数据。修改list会影响到真实数据，复制list只是复制指针，做修改会影响真实数据，需要deepcopy

- NumPy中的数据类型：dtype，类型转换要用.astype()

- NumPy索引：可以用二维花式索引

- 拷贝与视图：视图不复制原有数据，而拷贝则需要NumPy，有些索引会返回拷贝(花式索引)，有些是视图

- 布尔索引：any,all

- 条件筛选下标：二维要考虑轴的问题

- NumPy运算：

  **广播机制：尺寸不匹配时将小的广播到大的尺寸上**

  **广播机制的条件：==*两个向量维度相同(shape)*==  OR  ==*某个维度一个向量有，一个无*==  OR  ==某个维度一个向量有，一个size为1==**

  矩阵转置

  将矩阵“拍平”成一维矩阵

  矩阵的堆叠（合并），高维的时候使用==concatenate()==函数更方便

  矩阵的切分，自我重复，部分删除，插入，外边框增加

  点积和叉积，统计，矩阵排序

## Part 02 手写SIMD向量化

### SIMD

- 单指令多数据流
- x86下，SIMD一般和SSE和AVX等指令集联系在一起，SSE和AVX指令集中提供了大量可以单指令操作多个数据单元的指令
- 数据个数$\ne$加速倍数
- 指令处理的数据越长越好？:anger:不现实

**为什么要手写？**

简单情况 -O3 -mavx2，编译器会自动优化

代码结构复杂，循环难以界定边界，还有分支，只能手写

**怎么看文档？**查看[**Intel Intrinsic**官网](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#)

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20250705220744585.png" alt="image-20250705220744585" style="zoom:50%;" />

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20250705221044889.png" alt="image-20250705221044889" style="zoom:50%;" />

### 基本流程

Load需要计算的数据到向量寄存器，进行需要的向量化计算，Store将向量寄存器的数据存回内存

### 常见问题

- 内存对齐
- 循环边界不确定——展开非整数边界
- 循环分支的开销掩盖了SIMD提升——循环展开
- 寄存器数量有限——有多少寄存器用来计算

:warning:大多数情况自动向量化就够了，一定一定要==**最后**==在进行手写SIMD优化

## 优化矩阵乘法

1. 找到for循环，确定C矩阵和A，B矩阵
2. 确定矩阵中的变量类型
3. 找指令

Lab2是要找一个指令使得16bits*16bits得到32bits

 bonus：画图

### AMX

代码模板：

```cpp
for i = 0 to M step A_TILE_LINES:			//M < 16时可以去除此层for循环
    for j = 0 to N step B_TILE_COLS:		//A_TILE和B_TILE可以自己定义
        SET C_TILE ZERO
        for k = 0 to K step B_TILE_LINES:
            LOAD A_TILES
            LOAD B_TILES
            TILE_MATMUL
        STORE C_TILES
```

**重点是要对B矩阵进行reshape**

<img src="C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20250710163201917.png" alt="image-20250710163201917" style="zoom: 50%;" />

**Note:图中的一个小方块代表16bit，reshape的实质是对B矩阵进行以32bit为单位的转置**

示例：

```cpp
for (auto logical_block_id = logical_block_start;
             logical_block_id < logical_block_end;
             logical_block_id++) {
          auto physical_block_id = block_tables_ptr
              [seq_id * max_num_blocks_per_seq + logical_block_id];
          auto tokens_in_block =
              std::min(block_size, context_len - logical_block_id * block_size);
          auto token_start = logical_block_id * block_size;
          auto token_end = token_start + tokens_in_block;
          for (auto token_id = token_start; token_id < token_end; token_id++) {
            auto block_offset = token_id - token_start;
            auto k_cache_start = key_cache_ptr +
                physical_block_id * kv_block_strideN +
                block_offset * kv_block_strideP + kv_head_id * kv_block_strideH;
            for (auto i = 0; i < kv_head_group_size(M); i++) {
                logits[i * PARTITION_SIZE + logits_position] = 0;
                for (auto hsi(k) = 0; hsi < head_size(K); hsi++) {
                logits[i * PARTITION_SIZE + logits_position] +=
                    (float)q_ptr_start[i * head_size + hsi] * (float)k_cache_start[block_offset 
                    * kv_block_strideP + hsi];
                }
            }
            logits_position++;
          }
        }
```

AMX优化结果：

```cpp
// A q_ptr_start kv_head_group_size * head_size
// B k_cache_start block_size * kv_block_strideP
// reshape_B head_size / (4 / sizeof(cache_t)) * block_size * (4 / sizeof(cache_t)) 
// C logits_ptr kv_head_group_size * PARTITION_SIZE
// kv_head_group_size = 5
if (tokens_in_block == block_size)
{
// printf("kv_head_group_size: %d, head_size: %d, block_size: %d\n", kv_head_group_size, head_size, block_size);
	reshape((uint8_t*)k_cache_start, block_size, head_size * sizeof(cache_t), (uint8_t*)reshape_B, kv_block_strideP * sizeof(cache_t)); // reshape matrix B
	for (auto j = 0; j < block_size; j += 32) {
        _tile_zero(6);
        _tile_zero(7);
        for (auto k = 0; k < head_size; k += 128 / sizeof(cache_t)) {
            _tile_loadd(0, q_ptr_start + k, head_size * sizeof(scalar_t)); //参数（目标寄存器编号，第一个头对应的地址，stride）stride = A矩阵列个数*每个元素的size
            //此处load的大小是事先定好的，AMX寄存器最多可以load 1024 byte，但是最多16行，64列
            _tile_loadd(1, q_ptr_start + k + 64 / sizeof(cache_t), head_size * sizeof(scalar_t));
            _tile_loadd(2, reshape_B + k * block_size + j * (4 / sizeof(cache_t)), block_size * 4);
            _tile_loadd(3, reshape_B + k * block_size + (j + 16) * (4 / sizeof(cache_t)), block_size * 4);
            _tile_loadd(4, reshape_B + (k + 64 / sizeof(cache_t)) * block_size + j * (4 / sizeof(cache_t)), block_size * 4);
            _tile_loadd(5, reshape_B + (k + 64 / sizeof(cache_t)) * block_size + (j + 16) * (4 / sizeof(cache_t)), block_size * 4);

            _tile_dpbf16ps(6, 0, 2);
            _tile_dpbf16ps(7, 0, 3);
            _tile_dpbf16ps(6, 1, 4);
            _tile_dpbf16ps(7, 1, 5);
    	}
    _tile_stored(6, logits_ptr + j, PARTITION_SIZE * 4);
    _tile_stored(7, logits_ptr + j + 16, PARTITION_SIZE * 4);
    }
    logits_position += block_size;
}
```



**实际应用过程中大部分可以用AMX，无法整除的部分用AVX**